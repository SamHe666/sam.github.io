---
layout: post
title: "NLP-秦曾昌 Chapter 3"
date:   2020-11-05
tags: [NLP]
toc: true
author: Sam
---



## 自然语言

### 1. 语言和语义的进化

语言不是一成不变的，它会随着社会的发展会发生一些变化（比如说，一些不常用的词或者语言习惯会消亡或被替代）或者说被赋予新的含义（比如说apple）。现在自然语言处理主要集中在字面的处理，对背后的语义如何建模是一个难点，尤其是解读像成语或者新的语言背后的含义，这更是给自然语言处理带来了很大的挑战。

 

### 2. 香农信息论

#### 2.1. 信息熵

熵（Entropy）源自于物理学，描述了物体的混乱状态。

最早研究信息论的是C.SHANNON。在信息论里面，熵其实表示一个事件所含信息量的大小。越不可能发生的事，信息量越大。对于一个一定会发生的事件，其发生概率为1，信息量为0。假设X代表是一个事件，比如说是一个电话或者一封邮件。那么我们如何衡量X的信息量 $H(X)$ 的大小呢？

按照常理来说，对于$H(X)$，我们认为它应该满足以下关系：
1. $H(X) \propto \frac{1}{P(X)}$, 事件出现概率越小，它的信息量越大。一个例子就是，我告诉A，B是B他妈生的。这件事的信息量为0。
2. $H(X_1, X_2) = H(X_1) + H(X_2)$, 2件独立事情同时发生的信息量等于每件事件的信息量的加和
3. $H(X) \geq 0$, 信息量是一个正数

研究发现，$H(X) = log\frac{1}{P(X)} = -log P(X)$ 可以同时满足以上3个条件。

完整的信息熵的定义就是单个事件信息熵的数学期望：
$$
Entropy(X) = E_X[H(X)] = -\sum_X P(X)logP(X)
$$


#### 2.2. 交叉熵

##### 2.2.1. KL散度与交叉熵

要弄清楚交叉熵，必须先了解清楚KL散度。总的来说，KL散度是用来衡量2个事件或者2个分布的不同。KL散度不具备有对称性。假设 $A$，$B$ 是关于随机变量$X$的2个分布，则我们有：
$$
D_{K L}(A \| B)=\sum_{i} P_{A}\left(x_{i}\right) \log \left(\frac{P_{A}\left(x_{i}\right)}{P_{B}\left(x_{i}\right)}\right)=\sum_{i} P_{A}\left(x_{i}\right) \log \left(P_{A}\left(x_{i}\right)\right)-P_{A}\left(x_{i}\right) \log \left(P_{B}\left(x_{i}\right)\right)
$$
可以发现，其实减号左边的就是事件A的熵，而减号右边其实就是交叉熵。换句话说，KL散度由A自己的熵与B在A上的期望共同决定。事实上，KL散度 = 交叉熵 – 熵。也就是说，交叉熵的定义为：
$$
H(A, B)=-\sum_{i} P_{A}\left(x_{i}\right) \log \left(P_{B}\left(x_{i}\right)\right)
$$
此处最重要的观察是，KL散度和交叉熵在特定条件下（如果分布 $A$ 已知的话）等价。



##### 2.2.2. 机器学习与交叉熵

那么，应该怎么从机器学习的角度去理解这件事呢？在机器学习中，我们希望模型学到的分布 $P_{model}(X)$ 和真实数据的分布 $P_{real}(X)$ 越接近越好。模型学到的分布 $P_{model}(X)$ 和真实数据的分布 $P_{training}(X)$ 越接近越好。这里有一个很重要的假设，就是训练样本是真实数据的独立同分布采样 （所以我们这里才能用 $P_{training}(X)$来表示 $P_{real}(X)$ ）。



最小化模型分布 $P_{model}(X)$ 与 训练数据上的分布P(training)的差异等价于最小化这两个分布间的KL散度，也就是最小化 $D_{K L}(P_{training} \| P_{model})$ 。结合我们上面的公式来说的话，就是 $A$ 是training，$B$ 是model。因为 $A$ 是训练数据，分布已知，所以 $A$ 的期望熵已知，所以求 $D_{K L}(P_{training} \| P_{model})$ 等价于求 $A$ 和 $B$ 的交叉熵。



举个例子： 如下表所示，computed 一栏是预测结果，targets 是预期结果。 二者的数字，都可以理解为概率。

**模型1：**

| COMPUTED    | TARGETS            | CORRECT? |
| :---------- | :----------------- | :------- |
| 0.3 0.3 0.4 | 0 0 1 (democrat)   | yes      |
| 0.3 0.4 0.3 | 0 1 0 (republican) | yes      |
| 0.1 0.2 0.7 | 1 0 0 (other)      | no       |

那么，第一个训练样本的交叉熵为：
$$
-((\ln (0.3) * 0)+(\ln (0.3) * 0)+(\ln (0.4) * 1))=-\ln (0.4)
$$
所有样本的交叉熵之和为：
$$
-(\ln (0.4)+\ln (0.4)+\ln (0.1)) / 3=1.38
$$


**注意，基本所有分类问题都用one hot encoding + cross entropy**



##### 2.2.3. 反问

1. 为什么不能用classification error来做cost呢？因为不够精确，没办法判断哪个模型更接近。假设我们有个模型2.

   **模型 2**：

   | COMPUTED    | TARGETS            | CORRECT? |
   | :---------- | :----------------- | :------- |
   | 0.1 0.2 0.7 | 0 0 1 (democrat)   | yes      |
   | 0.1 0.7 0.2 | 0 1 0 (republican) | yes      |
   | 0.3 0.4 0.3 | 1 0 0 (other)      | no       |

   **结论：**

   2 个模型的 classification error 相等，都是1/3，但模型 2 要明显优于模型 1。所以说，Classification error 很难精确描述模型与理想模型之间的距离。

   

2. 为什么不能用平方和来做cost呢？像regression一样？因为分类模型的输出是类别的概率，通常是softmax输出的，如果用平方和来做cost，那么求导的时候函数就是非凸的了。参考链接: https://jackon.me/posts/why-use-cross-entropy-error-for-loss-function/ 
   $$
   \text { softmax }(x)_{i}=\frac{\exp \left(x_{i}\right)}{\sum_{j} \exp \left(x_{j}\right)}
   $$
   

### 3. 语言模型

#### 3.1. 定义

语言模型 Language Modelling（LM）最初是针对语音识别问题而开发的，现在广泛用于其他 NLP 应用中，比如机器翻译需要利用 LM 来给翻译出的句子打分。

假设我们有一个广义语句 $S=W_{1}, W_{2}, \ldots, W_{k}$， 其中$W_k$表示单词。那么，语言模型就是计算这个单词序列的概率。从机器学习的角度来看：语言模型是对语句的**概率分布**的建模。

 

#### 3.2. 词袋模型

认为词与词之间没啥联系， 不合理。更应该认为为条件概率。

但是仍然有用。用来统计词频。

补充它的优缺点。



**例子**

情感分析，logistic regression

x是词频，其实是tf-idf值

w可以理解为情感权重，正数越大，正情感越大，反之亦然，w接近0表示中性词。这个值得注意，因为平时我们没太关注lr的权重的意义。

查看分布会发现，非常正和非常负的占比很少，大量的词其实没有太强烈的情绪。符合语言假设