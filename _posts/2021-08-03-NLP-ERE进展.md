---
layout: post
title: "NLP-ERE进展-1"
date:   2021-08-03
tags: [NLP]
toc: true
author: Sam
---



### NER的4种标注框架

1. 序列标注

   每个序列位置都被标注一个标签，比如按照BILOU标注，然后用CRF解码。序列标注的方式对噪声（漏标）十分敏感，但是这种情况比较常见。一个解决方案就是采用下面说的片段排列，转化为一个span分类问题，这样更适合对负样本实体的采样；这样模型建模不会像对序列标注中的漏标过于敏感，也更好控制。此外，序列标注没办法解决嵌套问题，一个改进方法是采取token-level 的多label分类，将SoftMax替换为Sigmoid，但是这样没办法用CRF解码（因为CRF输入的是所有标签的概率分布，而不是每个标签是否入选的概率）。所以这种方式可能会导致label之间依赖关系的缺失，可采取后处理规则进行约束。

2. 指针标注

   利用指针网络给每个位置给一个start|None或者end|None的二分类标签，用以标注一个实体的起始位置。如果是多类别实体，那么就转化为层叠式指针标注：C类实体，C个指针网络。但是Pointer Network一般是用两个模块分别识别实体的首和尾，这会带来训练和预测时的不一致。指针网络对实体的长度和跨度都不会特别敏感，很容易会把意两个实体的首尾组合都当成目标预测出来。指针网络最早其实是应用于MRC中，而MRC中通常根据1个question从passage中抽取1个答案片段，转化为**2个n元SoftMax分类**（输入整个文档，输出哪个位置是头或者尾指针，每个位置是一个类）预测头指针和尾指针。但是对于NER可能会存在多个实体Span，因此需要转化为**n个2元Sigmoid分类**预测头指针和尾指针。但是，假设序列长度为n，那么每个指针网络就要做n个二分类，这样会导致样本Tag空间稀疏，同时收敛速度会较慢，特别是对于n比较大的情况。

3. 多头标注

   对每个token pair进行标记，构建一个$$N \times N \times C$$的分类矩阵，可以用于实体或者关系抽取。这个相比较与指针网络，综合考虑了实体首尾的一致性。事实上，**多头标注成为了众多实体和关系抽取SOTA的首选利器**！

4. 片段排列

   枚举所有可能的span进行分类，与序列长度进行解耦，可以更加灵活地处理复杂抽取和低资源问题。事实上，**片段排列的思想已经被Google推崇[3]并统一了信息抽取各个子任务**。对于含T个token的文本，理论上共有$$N = T(T+1)/2$$​种片段排列。如果文本过长，会产生大量的负样本，在实际中需要限制span长度并合理削减负样本。需要注意的是：

   - 实体span的编码表示：在span范围内采取注意力机制与基于原始输入的LSTM编码进行交互。然后所有的实体span表示并行的喂入SoftMax进行实体分类。
   - 这种片段排列的方式对于长文本复杂度是较高的。
   
   

### 关系抽取概览

**实体关系抽取**（Entity and Relation Extraction，**ERE**）是信息抽取的关键任务之一。ERE是级联任务，分为两个子任务：实体抽取和关系抽取。

1. 关系抽取范式主要由2大类：

   ![img](D:\lls\hejinyuan\Desktop\samhe666.github.io\images\post\2021-08-03-NLP-ERE进展\format,png)

2. Pipeline方法指先抽取实体、再抽取关系。相比于传统的Pipeline方法，联合抽取能获得更好的性能。虽然Pipeline方法易于实现，这两个抽取模型的灵活性高，实体模型和关系模型可以使用独立的数据集，并不需要同时标注实体和关系的数据集。但存在以下缺点：

   - 误差积累：实体抽取的错误会影响下一步关系抽取的性能。
   - 实体冗余：由于先对抽取的实体进行两两配对，然后再进行关系分类，没有关系的候选实体对所带来的冗余信息，会提升错误率、增加计算复杂度。
   - 交互缺失：忽略了这两个任务之间的内在联系和依赖关系。

   而基于共享参数的联合抽取方法仍然存在训练和推断时的gap，推断时仍然存在误差积累问题，可以说只是缓解了误差积累问题。

3. Joint方法指的是联合模型，将两个子模型统一建模。联合抽取的**难点**是如何加强实体模型和关系模型之间的交互，比如实体模型和关系模型的输出之间存在着一定的约束，在建模的时候考虑到此类约束将有助于联合模型的性能。现有联合抽取模型总体上有两大类：

   1. **共享参数**的联合抽取模型

      通过共享参数（共享输入特征或者内部隐层状态）实现联合，此种方法对子模型没有限制，但是由于使用独立的解码算法，导致实体模型和关系模型之间交互不强。绝大数文献还是基于参数共享进行联合抽取的。

   2. **联合解码**的联合抽取模型



### 基于共享参数的联合抽取方法

在联合抽取中的实体和关系抽取的解码方式与Q2中的实体抽取的解码方式基本一致，主要包括：序列标注CRF/SoftMax、指针网络、分类SoftMax、Seq2Seq等。基于共享参数的联合抽取，实体抽取loss会与关系抽取loss相加。

由于很多的相关文献实用性不高，这里只介绍其中具备代表性和易于应用的几篇文献，首先归纳如下：

| 序号/文献 | 联合抽取顺序 | 实体抽取解码方法 | 关系抽取解码方法 |
| --------- | ------------ | ---------------- | ---------------- |
| 1: [3]    |              |                  |                  |
| 2: [4]    |              |                  |                  |
| 3: [5]    |              |                  |                  |
| 4: [6]    |              |                  |                  |
| 5: [7]    |              |                  |                  |
| 6: [8]    |              |                  |                  |
| 7: [9]    |              |                  |                  |
| 8: [10]   |              |                  |                  |



##### 1. End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures

##### 2. Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees

##### 3. Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism

##### 4. Joint entity recognition and relation extraction as a multi-head selection problem

##### 5. Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy

##### 6. Entity-Relation Extraction as Multi-Turn Question Answering

##### 7. Span-Level Model for Relation Extraction 

##### 8. Span-based Joint Entity and Relation Extraction with Transformer Pre-training



### Reference

1. [一人之力，刷爆三路榜单！信息抽取竞赛夺冠经验分享](https://blog.csdn.net/xixiaoyaoww/article/details/110508476?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162201226516780265487362%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=162201226516780265487362&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v2~rank_v29-7-110508476.pc_v2_rank_blog_default&utm_term=ner&spm=1018.2226.3001.4450)
2. nlp中的实体关系抽取方法总结：https://zhuanlan.zhihu.com/p/77868938
3. End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures https://www.aclweb.org/anthology/P16-1105.pdf
4. Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees https://pdfs.semanticscholar.org/bbbd/45338fbd85b0bacf23918bb77107f4cfb69e.pdf?_ga=2.119149259.311990779.1584453795-1756505226.1584453795
5. Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism https://aclanthology.org/P18-1047.pdf
6. Joint entity recognition and relation extraction as a multi-head selection problem https://arxiv.org/abs/1804.07847
7. Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy https://arxiv.org/abs/1909.04273
8. Entity-Relation Extraction as Multi-Turn Question Answering https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1905.05529.pdf
9. Span-Level Model for Relation Extraction https://www.aclweb.org/anthology/P19-1525.pdf
10. Span-based Joint Entity and Relation Extraction with Transformer Pre-training https://arxiv.org/pdf/1909.07755.pdf

