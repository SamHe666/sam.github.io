---
layout: post
title: "NLP-秦曾昌 Chapter 7 & 8"
date:   2020-11-22
tags: [NLP]
toc: true
author: Sam
---

## 语言技术 - 主题模型

### 1. 概率图模型

虽然今天用深度学习比较多，但是对于小文本、数据量不是特别大的情况，graphic model还是能提供很多有用的思路去解决问题。



其实概率图模型不是什么新东西，其实就是用图的形式来表达一个概率模型。就算是一个简单的条件概率或者是较为复杂的隐马尔科夫链模型也都可以用图来表示。图的表示方法可以让你更好的理解整个模型，比如说直观地看到变量的依赖关系啊，一个变量由什么变量决定的，那些是观测变量，那些是隐含变量等等。



### 2. 主题模型 

在自然语言理解任务中，我们可以通过一系列的层次来提取含义——从单词、句子、段落，再到文档。在文档层面，理解文本最有效的方式之一就是分析其主题。在文档集合中学习、识别和提取这些主题的过程被称为主题建模。总的来说，主题模型可以用于文档的语义挖掘，排除文档中噪音的影响，解决多义词的问题，计算文档相似度，文本分类，推荐系统（找到相似主题的文章）等等。主题模型是语言无关的。任何语言只要能够对它进行分词，就可以进行训练（无监督），得到它的主题分布。



所有主题模型都基于相同的基本假设：

- 每个文档包含多个主题
- 每个主题包含多个单词



换句话说，主题模型围绕着以下观点构建：实际上，文档的语义由一些我们所忽视的隐变量或「潜」变量管理。因此，主题建模的目标就是揭示这些潜在变量——也就是主题，正是它们塑造了我们文档和语料库的含义。

#### 2.1. LSA

LSA: Latent Semantic Analysis 潜在语义分析。



**算法思路：**

1. 生成文档-词语矩阵：如果在词汇表中给出 m 个文档和 n 个单词，我们可以构造一个 m×n 的矩阵 A，其中每行代表一个文档，每列代表一个单词。矩阵中的每个元素可以是该单词在对应文档的计数，也可以是单词的TF-IDF值。

2. 问题：A 极有可能非常稀疏、噪声很大，并且在很多维度上非常冗余。解决：使用截断 SVD来降维。
   $$
   A \approx A^{\prime} = U_{t} S_{t} V_{t}^{T}
   $$
   直观来说，截断 SVD 可以看作只保留我们变换空间中最重要的 t 维。

   ![img](https://i.loli.net/2020/11/23/Iqr2DNgX5c6j98K.png)

   

3. 在这种情况下，$U \in R^{m⨉t}$是我们的文档-主题矩阵，而 $V \in R^{n⨉t}$则成为我们的词语-主题矩阵。在矩阵 U 和 V 中，每一列对应于我们 t 个主题当中的一个。在 U 中，行表示**按主题表达**的文档向量；在 V 中，行代表**按主题表达**的术语向量。

4. 通过这些文档向量和术语向量，现在我们可以轻松应用**余弦相似度**等度量来评估以下指标：

   - 不同文档的相似度
   - 不同单词的相似度
   - 单词（或「queries」）与文档的相似度



LSA优点是快速且高效，但是也有**主要缺点：**

1. 缺乏可解释的嵌入（我们并不知道主题是什么，其成分可能积极或消极，这一点是随机的）
2. 需要大量的文件和词汇来获得准确的结果
3. SVD的计算复杂度很高，而且当有新的文档来到时，若要更新模型需重新训练。
4. 表征效率低



#### 2.2. PLSA 

PLSA: Probabilistic Latent Semantic Analysis 概率潜在语义分析。

PLSA采取概率方法替代 SVD 以解决问题。其核心思想是找到生成文档-词语矩阵中观察到的数据的概率模型P(D,W)。PLSA有一个重要的假设就是**词袋假设**，即认为一篇文档中的单词是可以交换次序的而不影响模型的训练结果。



回想一下主题模型的基本假设：每个文档由多个主题组成，每个主题由多个单词组成。PLSA 为这些假设增加了概率自旋：

- 给定文档 d，主题 z 以 $P(z \mid d)$ 的概率出现在该文档中

- 给定主题 z，单词 w 以 $P(w \mid z)$ 的概率从主题 z 中提取出来

  ![img](https://i.loli.net/2020/11/23/pdSaUVTm1l954AG.png)

所以，从形式上来看，一个给定的文档和单词同时出现的联合概率是：
$$
P(d_i, w_j)=P(d_i) \sum_{z} P(z \mid d_i) P(w_j \mid z)
$$
其中，$P(d_i)$ 可以直接由我们的语料库确定。$P(z \mid d_i)$ 和 $P(w_j \mid z)$ 利用了多项式分布建模。



我们可以通过极大似然来建立PLSA的目标函数：
$$
L=\prod_{i=1}^{N} \prod_{j=1}^{M} P\left(d_{i}, w_{j}\right)=\prod_{i} \prod_{j} P\left(d_{i}, w_{j}\right)^{n\left(d_{i}, w_{j}\right)}
$$
其中 $n\left(d_{i}, w_{j}\right)$ 表示 $w_j$ 在文档 $d_i$ 中出现的次数。对目标函数两边取对数得：
$$
log \ L=\sum_{i} \sum_{j} n\left(d_{i}, w_{j}\right) \log \left[\sum_{k=1}^{K} P\left(w_{j} \mid z_{k}\right) P\left(z_{k} \mid d_{i}\right) P\left(d_{i}\right)\right]
$$
可以看出，对于上面的目标函数，如果做极大似然估计，通过导数等于0的方式求出极大的参数，显然是行不通的，因为：

1. 拥有隐变量z；  
2. log中有加法。

所以呢，我们需要利用EM算法来求解。具体求解可以参考Ref 2-3。不同于典型的求 $z$ 的后验概率，这里E步求解的是 $z \mid d$ 的后验概率。其实方法和思路是一样的，因为d是个已知量。求解过程中还涉及到等式约束的求极值问题，需要使用Lagrange乘子法解决。这个到使用的时候再细究了。



PLSA**缺点**：

1. 没有参数来给 P(D) 建模，所以不知道如何为新文档分配概率。必须在确定document 的情况下才能对模型进行随机抽样。
2. PLSA 的参数数量随着我们拥有的文档数线性增长，因此容易出现过度拟合问题



很少会单独使用 PLSA。一般来说，当人们在寻找超出 LSA 基准性能的主题模型时，他们会转而使用 LDA 模型。LDA 是最常见的主题模型，它在 PLSA 的基础上进行了扩展，从而解决这些问题。



#### 2.3. LDA



### Reference

1. https://www.jiqizhixin.com/articles/topic-modeling-with-lsa-psla-lda-and-lda2vec
2. https://zhuanlan.zhihu.com/p/40877820
3. https://zhuanlan.zhihu.com/p/56265390